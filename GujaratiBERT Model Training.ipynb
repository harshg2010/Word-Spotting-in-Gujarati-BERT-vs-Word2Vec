{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-21T02:42:15.653425Z","iopub.status.busy":"2024-03-21T02:42:15.652874Z","iopub.status.idle":"2024-03-21T02:42:15.663686Z","shell.execute_reply":"2024-03-21T02:42:15.662070Z","shell.execute_reply.started":"2024-03-21T02:42:15.653390Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/bertmodel/config.json\n","/kaggle/input/bertmodel/README.md\n","/kaggle/input/bertmodel/tokenizer.json\n","/kaggle/input/bertmodel/tokenizer_config.json\n","/kaggle/input/bertmodel/pytorch_model.bin\n","/kaggle/input/bertmodel/model.safetensors\n","/kaggle/input/bertmodel/special_tokens_map.json\n","/kaggle/input/bertmodel/vocab.txt\n","/kaggle/input/selected-data-csv/selected_data.csv\n","/kaggle/input/gujarati-filtered-data/filtered_data.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T03:10:36.291467Z","iopub.status.busy":"2024-03-18T03:10:36.290512Z","iopub.status.idle":"2024-03-18T03:10:36.836485Z","shell.execute_reply":"2024-03-18T03:10:36.835401Z","shell.execute_reply.started":"2024-03-18T03:10:36.291426Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import random\n","\n","# Load the dataset\n","dataset = pd.read_csv('/kaggle/input/gujarati-filtered-data/filtered_data.csv')\n","\n","\n","# Display the selected data\n","print(dataset)\n","print(dataset.shape)\n","\n","\n","print(test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T06:11:20.367578Z","iopub.status.busy":"2024-03-16T06:11:20.367031Z","iopub.status.idle":"2024-03-16T06:11:58.439594Z","shell.execute_reply":"2024-03-16T06:11:58.437878Z","shell.execute_reply.started":"2024-03-16T06:11:20.367549Z"},"trusted":true},"outputs":[],"source":["pip install transformers\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculate accuracy\n","#         _,predicted_indices = torch.max(logits, 2)\n","#         print(\"predicted_indices\",predicted_indices)\n","#         print(\"target_indices\",target_indices.view)\n","#         correct_predictions += torch.sum(torch.eq(predicted_indices, target_indices.view(-1, 1))).item()\n","#         total_predictions += target_indices.size(0)\n","        \n","#         print(\"total_predictions\",total_predictions)\n","#         print(\"target_indices_size\",len(target_indices))\n","\n","#          # Calculate precision\n","#         predicted_word_indices = predicted_indices.squeeze().tolist()  # Convert to list\n","#         target_word_indices = target_indices.squeeze().tolist()  # Convert to list\n","#         total_target_occurrences += len(target_word_indices)\n","\n","#         for predicted_idx in predicted_word_indices:\n","#             if predicted_idx in target_word_indices:\n","#                 correct_predictions += 1\n","\n","#         precision = correct_predictions / (total_predictions + 1e-8) # Avoid division by zero\n","#         print(\"Precision\",precision)\n","\n","# def validate(model, dataloader, device):\n","#     model.eval()\n","#     total_loss = 0\n","#     correct_predictions = 0\n","#     total_predictions = 0\n","\n","#     with torch.no_grad():\n","#         for batch in tqdm(dataloader, desc=\"Validation\"):\n","#             input_ids = batch['input_ids'].to(device)\n","#             attention_mask = batch['attention_mask'].to(device)\n","#             target_indices = batch['target_indices'].squeeze().to(device)\n","\n","#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","#             logits = outputs.logits\n","\n","#             # Calculate loss for target word positions\n","#             loss = torch.mean(logits[0, target_indices])\n","#             total_loss += loss.item()\n","#             print(total_loss)\n","\n","#             # Make predictions and track accuracy\n","#             predicted_indices = logits.argmax(dim=2)\n","#             correct_predictions += torch.sum(predicted_indices == target_indices.unsqueeze(-1)).item()\n","#             total_predictions += target_indices.numel()\n","\n","#     avg_loss = total_loss / len(dataloader)\n","#     accuracy = correct_predictions / total_predictions\n","\n","#     print(f\"Validation Avg Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T02:43:26.153883Z","iopub.status.busy":"2024-03-21T02:43:26.153456Z","iopub.status.idle":"2024-03-21T02:43:27.423476Z","shell.execute_reply":"2024-03-21T02:43:27.421420Z","shell.execute_reply.started":"2024-03-21T02:43:26.153851Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["                                                    text target_word  \\\n","0      1 એપ્રિલ 2007ના દિવસે નોકિયાના નેટવર્ક્સ બિઝને...        સાથે   \n","1      મહાવીર સ્વામી, છેલ્લા તિર્થંકર, ક્ષત્રિય કુળમા...     વર્ણનાં   \n","2      બુદ્ધિપ્રકાશ ‍ ગુજરાતી ભાષાનું ગુજરાત વિદ્યા સ...     ગુજરાતી   \n","3                               ગુજરાત અને ભારતમાં સ્થાન         અને   \n","4                               ગુજરાત અને ભારતમાં સ્થાન       સ્થાન   \n","...                                                  ...         ...   \n","40727  આ જિલ્લો ૩૮૫૦ ચોરસ કિલોમીટર જેટલો વિસ્તાર ધરાવ...   જિલ્લામાં   \n","40728  સાલેજ ભારત દેશના પશ્ચિમ ભાગમાં આવેલા ગુજરાત રા...        જેવી   \n","40730  3,061 metres જેટલી લંબાઈનો માર્ગ ધરાવતો આ સેતુ...           આ   \n","40735  હવા પ્રદુષણ મોટેભાગે વધુ વસ્તીવાળા શહેરી વિસ્ત...     દેશોમાં   \n","40736  આઉટસીડ ઈન્સીડ માં લેસબિયન, ગે અને બાયસેક્સ્યુઅ...       જાતિય   \n","\n","       word_count  \n","0              34  \n","1              37  \n","2              14  \n","3               4  \n","4               4  \n","...           ...  \n","40727          38  \n","40728          45  \n","40730          32  \n","40735          38  \n","40736          28  \n","\n","[23667 rows x 3 columns]\n","(23667, 3)\n"]}],"source":["import pandas as pd\n","\n","# Load the dataset\n","dataset = pd.read_csv('/kaggle/input/gujarati-filtered-data/filtered_data.csv')\n","\n","# dataset = dataset[0:15]\n","\n","dataset['word_count'] = dataset['text'].apply(lambda x: len(x.split()))\n","dataset = dataset[(dataset['word_count'] >= 3) & (dataset['word_count'] < 63)]\n","\n","# Display the filtered data and its shape\n","print(dataset)\n","print(dataset.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T13:26:44.412029Z","iopub.status.busy":"2024-03-19T13:26:44.41162Z","iopub.status.idle":"2024-03-19T14:10:42.337571Z","shell.execute_reply":"2024-03-19T14:10:42.336694Z","shell.execute_reply.started":"2024-03-19T13:26:44.411978Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import AutoTokenizer, BertForMaskedLM\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","import torch.nn.functional as F\n","\n","# Define dataset class\n","class GujaratiWordSpottingDataset(Dataset):\n","    def __init__(self, texts, target_words, tokenizer, max_length=128):\n","        self.texts = texts\n","        self.target_words = target_words\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = str(self.texts[idx])\n","        target_word = str(self.target_words[idx])\n","\n","        # Tokenize inputs\n","        inputs = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","            truncation=True\n","        )\n","\n","        # Tokenize target word\n","        target_token = self.tokenizer.tokenize(target_word)\n","\n","        target_indices = []\n","        if target_token:  # Check if target_token is not empty\n","            for i in range(len(inputs['input_ids'][0])):\n","                token_str = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][i].item())\n","                if token_str == target_token[0]:\n","                    target_indices.append(i)\n","        else:\n","            # If target_token is empty, return empty tensors\n","            return {\n","                'input_ids': torch.tensor([]),\n","                'attention_mask': torch.tensor([]),\n","                'target_indices': torch.tensor([])\n","            }\n","\n","        return {\n","            'input_ids': inputs['input_ids'].squeeze(0),\n","            'attention_mask': inputs['attention_mask'].squeeze(0),\n","            'target_indices': torch.tensor(target_indices)\n","        }\n","\n","\n","# Load dataset\n","texts = dataset['text'].tolist()\n","target_words = dataset['target_word'].tolist()\n","\n","# Define paths to the downloaded model and tokenizer\n","model_path = \"/kaggle/input/bertmodel\"\n","tokenizer_path = \"/kaggle/input/bertmodel\"\n","\n","# Load the tokenizer and model from the downloaded files\n","tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, use_fast=False)\n","model = BertForMaskedLM.from_pretrained(model_path)\n","\n","# Prepare dataset and dataloader\n","dataset = GujaratiWordSpottingDataset(texts, target_words, tokenizer)\n","train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=42)  # Split dataset into train and validation\n","train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n","\n","# Define optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n","\n","# Training loop\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","model.train()\n","\n","for epoch in range(10):\n","    total_loss = 0\n","\n","    # Use tqdm for progress reporting\n","    with tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\", unit=\"batch\") as tepoch:\n","        for batch_idx, batch in enumerate(tepoch):\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            target_indices = batch['target_indices'].squeeze().to(device)\n","\n","            if target_indices.dim() == 0 or target_indices.numel() == 0:\n","                continue  # Skip this batch\n","\n","            if target_indices.dim() == 0:\n","                # Ensure target_indices has at least one dimension\n","                target_indices = target_indices.unsqueeze(0)  \n","            optimizer.zero_grad()\n","\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","            logits = outputs.logits\n","\n","            # Calculate loss for each target index\n","            loss = 0\n","            for idx in target_indices:\n","                loss += F.cross_entropy(logits[:, idx, :], idx.unsqueeze(0).to(device))\n","\n","            # Backpropagation\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","\n","            # Update tqdm description\n","            tepoch.set_postfix(loss=loss.item())\n","\n","    # Calculate average \n","    avg_loss = total_loss / len(train_dataloader)\n","\n","    print(f\"Epoch {epoch + 1}, Train Avg Loss: {avg_loss:.4f}\")\n","      \n","\n","# Save the model and tokenizer\n","model.save_pretrained(\"gujarati_word_spotting_model\")\n","tokenizer.save_pretrained(\"gujarati_word_spotting_model\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4606022,"sourceId":7853498,"sourceType":"datasetVersion"},{"datasetId":4608454,"sourceId":7857047,"sourceType":"datasetVersion"},{"datasetId":4608672,"sourceId":7857334,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
